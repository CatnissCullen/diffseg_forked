{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright 2023 Google LLC\n",
        "\n",
        "Use of this source code is governed by an MIT-style\n",
        "license that can be found in the LICENSE file or at\n",
        "https://opensource.org/licenses/MIT.\n",
        "\n",
        "# Instructions\n",
        "Please run the following cells sequentially\n",
        "1. (Optional) Running 1b adds semantic labels and requires addtional resources (default to a second GPU).\n",
        "* The function relies on an additonal image captioning model, e.g., BLIP.  \n",
        "* The labels are nouns, extracted from the generated caption. \n",
        "* It merge masks sharing the same label. \n",
        "2. Add your own image and update ``image_path`` variable. \n",
        "3. Feel free to play with DiffSeg hyper-parameters such as the ``KL_THRESHOLD``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r52zlVDUJl_9"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ViBbqfx9un_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import nltk\n",
        "from transformers import AutoProcessor, TFBlipForConditionalGeneration\n",
        "from third_party.keras_cv.src.models.stable_diffusion.image_encoder import ImageEncoder\n",
        "from third_party.keras_cv.stable_diffusion import StableDiffusion \n",
        "from third_party.keras_cv.diffusion_model import SpatialTransformer\n",
        "from diffseg.utils import process_image, augmenter, vis_without_label, semantic_mask\n",
        "from diffseg.segmentor import DiffSeg\n",
        "\n",
        "is_noun = lambda pos: pos[:2] == 'NN'\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfCj5i1be890"
      },
      "source": [
        "# 1. Initialize SD Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8b6beipfCA3"
      },
      "outputs": [],
      "source": [
        "# Inialize Stable Diffusion Model on GPU:0 \n",
        "with tf.device('/GPU:0'):\n",
        "  image_encoder = ImageEncoder()\n",
        "  vae=tf.keras.Model(\n",
        "            image_encoder.input,\n",
        "            image_encoder.layers[-1].output,\n",
        "        )\n",
        "  model = StableDiffusion(img_width=512, img_height=512)\n",
        "blip = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1b. Initialize BLIP (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally initalize a BLIP captioning model on GPU:1\n",
        "with tf.device('/GPU:1'):\n",
        "    processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    blip = TFBlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Run Inference on Real Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The first time running this cell will be slow\n",
        "# because the model needs to download and loads pre-trained weights.\n",
        "\n",
        "image_path = \"./images/img5.jpeg\" # Specify the path to your image\n",
        "\n",
        "if blip is not None:\n",
        "  with tf.device('/GPU:1'):\n",
        "    inputs = processor(images=Image.open(image_path), return_tensors=\"tf\")\n",
        "    out = blip.generate(**inputs)\n",
        "    prompt = processor.decode(out[0], skip_special_tokens=True)\n",
        "    print(prompt)\n",
        "else:\n",
        "  prompt = None\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "  images = process_image(image_path)\n",
        "  images = augmenter(images)\n",
        "  latent = vae(tf.expand_dims(images, axis=0), training=False)\n",
        "  images, weight_64, weight_32, weight_16, weight_8, x_weights_64, x_weights_32, x_weights_16, x_weights_8 = model.text_to_image(\n",
        "    prompt,\n",
        "    batch_size=1,\n",
        "    latent=latent,\n",
        "    timestep=300\n",
        "  )\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Generate Segementation Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "KL_THRESHOLD = [0.9]*3 # KL_THRESHOLD controls the merging threshold\n",
        "NUM_POINTS = 16\n",
        "REFINEMENT = True\n",
        "\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "  segmentor = DiffSeg(KL_THRESHOLD, REFINEMENT, NUM_POINTS)\n",
        "  pred = segmentor.segment(weight_64, weight_32, weight_16, weight_8) # b x 512 x 512\n",
        "  if blip is not None:\n",
        "    tokenized = nltk.word_tokenize(prompt)\n",
        "    nouns = [(i,word) for i,(word, pos) in enumerate(nltk.pos_tag(tokenized)) if is_noun(pos)] \n",
        "\n",
        "  for i in range(len(images)):\n",
        "    if blip is not None:\n",
        "      x_weight = segmentor.aggregate_x_weights([x_weights_64[i],x_weights_32[i], x_weights_16[i], x_weights_8[i]],weight_ratio=[1.0,1.0,1.0,1.0])\n",
        "      label_to_mask = segmentor.get_semantics(pred[i], x_weight[i], nouns,voting=\"mean\")\n",
        "      semantic_mask(images[i], pred[i], label_to_mask)\n",
        "    vis_without_label(pred[i],images[i],num_class=len(set(pred[i].flatten())))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "r52zlVDUJl_9",
        "8s6-2GsMiEwi",
        "YqhfoJBLPDkj",
        "HkDuWW_S6mFK",
        "Ir_E8vlvmC3i",
        "Jq_UYaw9z9W1",
        "AeVPOortS0Xg",
        "drmyt0Zq-Yuf",
        "F_2rE9z3lJxp",
        "gwq2tPWBoUCs",
        "ZuURD2pAgE6S",
        "MhoxFI32WrsZ",
        "KfCj5i1be890",
        "i9M17URAGrRT",
        "ZMg1NFyON8ve",
        "2uO8K_INONGB",
        "n4Mmj0o4OYWu",
        "25FBAbAWOjQT",
        "zdgg-Yirfj1s",
        "aiNXdN1UckMN",
        "XBW0qHSxlF0Z",
        "G4ktTGCsHjZU",
        "JTOmRatCdsap",
        "mc1OqXkMsik4",
        "EVTDEno2spnw"
      ],
      "last_runtime": {
        "build_target": "//experimental/humaninterface/explorations/diffseg/colab_runtime:ml_notebook",
        "kind": "private"
      },
      "name": "diffusion_inference_coco_cityscapes.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1689088568669
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688922575491
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_coco.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1688833060440
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687892328479
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687837396445
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687364924074
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1687114629456
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_inference_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686761132459
        },
        {
          "file_id": "/piper/depot/google3/experimental/humaninterface/explorations/junjiaot/diffusion/diffusion_test.ipynb?workspaceId=junjiaot:scene::citc",
          "timestamp": 1686163689880
        },
        {
          "file_id": "1z1ERc9A5S1u5ci0dHLSoqDgPeXpUI_lY",
          "timestamp": 1686159970456
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
